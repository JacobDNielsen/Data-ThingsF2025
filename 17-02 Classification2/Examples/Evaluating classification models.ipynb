{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating classification models\n",
    "\n",
    "In this notebook, we will look at how to evaluate classification models in terms of the confusion matrix and measures such as accuracy, precision, and recall, as well as the ROC curve and AUC. \n",
    "\n",
    "We will, again, use the diabetes dataset that can be used to classify whether people have diabetes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_data = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "We saw last time that there was a very large variance for the K-Nearest-Neighbor classifier, in the sense that we got very different results dependent on the train-test split (based on what number we used for `random_state`). Let us first visualize that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = diab_data[\"Outcome\"]\n",
    "X = diab_data.drop(columns = [\"Age\", \"Outcome\"])\n",
    "acclist = [] \n",
    "\n",
    "random_seeds = np.random.randint(1000, size=50)\n",
    "for seed in random_seeds:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=seed)\n",
    "    knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn5.fit(X_train, y_train)\n",
    "    y_pred5_test = knn5.predict(X_test)\n",
    "    acclist.append({\"seed\": seed, \"Test accuracy\": accuracy_score(y_test, y_pred5_test)})\n",
    "\n",
    "accuracyDF = pd.DataFrame(acclist)\n",
    "accuracyDF[\"Test accuracy\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = accuracyDF, x = \"Test accuracy\")\n",
    "plt.title(\"The distribution of test accuracy for 50 runs of KNN with K=5\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try to use cross-validation instead. There are multiple ways of doing this, but one is to use the `cross_val_score` from Scikit-learn model_selection module. It makes k-fold cross validation - that is, it trains k different models and evaluate the accuracy on the k hold-out folds. Note that, it only returns the accuracy scores of each fold. We should then train the model afterward on the entire set. (Strictly speaking, we should have done a train test split first and only ran the cross-validation on this training set. Then trained the model on the entire training set, and then finally evaluated the model on the untouched test set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "scores = cross_val_score(knn5, X, y, cv = 5)  # cv = 5 means 5-fold cross validation\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use the mean of these as an unbiased estimate of the accuracy of our model on future unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try out with different K's to see how much variance we have in this estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kacclist = [] \n",
    "for k in range(2, 20):\n",
    "    scores = cross_val_score(knn5, X, y, cv = k)\n",
    "    kacclist.append({\"Folds\": k, \"Mean accuracy\": scores.mean()})\n",
    "\n",
    "kaccuracyDF = pd.DataFrame(kacclist)\n",
    "kaccuracyDF[\"Mean accuracy\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(kaccuracyDF[\"Folds\"], kaccuracyDF[\"Mean accuracy\"], label = \"Mean accurcay\", color='blue', marker='o', linestyle='solid')    \n",
    "plt.xlabel('Number of folds (k)')\n",
    "plt.ylabel('Mean acuracy')\n",
    "plt.title(\"Accuracy of different Ks\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data = kaccuracyDF, x = \"Mean accuracy\")\n",
    "plt.title(\"The distribution of mean accuracy for 2 to 20 k-fold cross-validation\")\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use 5-fold cross validation to chose a $K$ for the KNN classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knnSweepCrossValidation(X, y, maxK, folds=5):\n",
    "    kacclist = []\n",
    "    \n",
    "    for k in range(2, maxK):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)     \n",
    "        scores = cross_val_score(knn, X, y, cv = folds)\n",
    "        kacclist.append({\"K\": k, \"CV accuracy\": scores.mean()})\n",
    "\n",
    "    return pd.DataFrame(kacclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knnSweepCrossValidation(X, y, 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compare it to the `random_state=42` from last time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knnSweep(X, y, maxK):\n",
    "    accuracy_row_list = []\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "    for k in range(2, maxK):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k)\n",
    "        knn.fit(X_train, y_train)\n",
    "        y_pred_test = knn.predict(X_test)\n",
    "        accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "        accuracy_row_list.append({\"K\": k, \"Random42 test accuracy\": accuracy_test})\n",
    "\n",
    "    return pd.DataFrame(accuracy_row_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ran25 = knnSweep(X, y, 25)\n",
    "cv25 = knnSweepCrossValidation(X, y, 25)\n",
    "\n",
    "compareDF = pd.merge(ran25, cv25, on=\"K\")\n",
    "compareDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(compareDF[\"K\"], compareDF[\"Random42 test accuracy\"], label = 'Random 42 test accuracy', color='blue', marker='o', linestyle='solid')\n",
    "plt.plot(compareDF[\"K\"], compareDF[\"CV accuracy\"], label = 'CV accurcay', color='red', marker='o', linestyle='solid')\n",
    "    \n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title(\"Accuracy of different Ks\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly see much less variance in the cross-validation accuracy! There is still variation, but that is mainly due to the fact that we have a small dataset and KNN is a model type with high variance. But it looks like that $K=10$ is a sensible choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The confusion matrix\n",
    "\n",
    "we will now focus on a single model of KNN trained for $K=5$ and $K=10$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn5 = KNeighborsClassifier(n_neighbors=5)\n",
    "knn5.fit(X, y)\n",
    "y_pred5 = knn5.predict(X)\n",
    "\n",
    "knn10 = KNeighborsClassifier(n_neighbors=10)\n",
    "knn10.fit(X, y)\n",
    "y_pred10 = knn10.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `confusion_matrix` function from the Scikit-learn metrics submodule, to get a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, y_pred5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y, y_pred10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make it visually more appealing with a heatmap style plot using `ConfusionMatrixDisplay` also from Scikit-learn metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y, y_pred5)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y, y_pred10)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model for $K=10$ make less False Positive predictions (45 instead of 52) than the model for $K=5$. On the other hand, it makes much more False Negative predictions (136 instead of 98). So the $K=10$ model does not seem much better than the $K=5$ model. Let us calculate the various metrics to make a more detailed comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If one wants to look at percentages instead, we can normalize the values in the cells to sum to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y, y_pred10, normalize='all')).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics\n",
    "\n",
    "We can import all the revelant evaluation metrics from Scikit-learn metrics submodule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluationScoreDF = pd.DataFrame({\"K\": [5, 10],\n",
    "                                  \"Accuracy\": [accuracy_score(y, y_pred5), accuracy_score(y, y_pred10)],\n",
    "                                  \"Precision\": [precision_score(y, y_pred5), precision_score(y, y_pred10)],\n",
    "                                  \"Recall\": [recall_score(y, y_pred5), recall_score(y, y_pred10)],\n",
    "                                  \"F1\": [f1_score(y, y_pred5), f1_score(y, y_pred10)]})\n",
    "EvaluationScoreDF                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the $K=5$ actually seems to perform better on all metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC and AUC evaluation\n",
    "\n",
    "We will not look at the ROC curve and calculate AUC for the models. For this we will use our logistic regression model from last time as it naturally can give us class probabilities instead hard labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model = linear_model.LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diab_data[[\"Glucose\", \"BMI\"]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to get the class probabilities (of class 1). We can use the method `.predict_proba` on the logistic model object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how it returns two columns of probabilities! The first column is the probability of class 0, while the second column is the probabilities of class 1 (note how the rows sum to 1). Thus, to get the probabilities for class 1, we can take the second column (denoted 1 in Python - I think I mixed it up last time):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = logit_model.predict_proba(X)[:,1]\n",
    "y_pred = logit_model.predict(X)\n",
    "\n",
    "y_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the ROC curve, we well use the `roc_curve` from Scikit-learn metrics to calculate the FPR and TPR for us that we can then plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the AUC directly using the `roc_auc_score` function from Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y, y_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
