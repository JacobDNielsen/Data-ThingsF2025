{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc949f0-1abd-4773-a16a-01e6b14998c0",
   "metadata": {},
   "source": [
    "# Decision trees and ensemble models\n",
    "\n",
    "In this notebook, we will look at Decision trees for classification as well as various ensemble models based on decision trees, such as Random Forest and AdaBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123594b0-9f19-47fb-baf0-05e49e4cae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252eb0c4-47db-49f6-a887-ff22e96ab1bc",
   "metadata": {},
   "source": [
    "We will again use the diabetes dataset as example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37918df4-f7bd-49e3-bfa9-d7a754960e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_data = pd.read_csv('diabetes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb11f7f-3621-474e-ae84-2cbc898e1ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d4e144-808b-460d-9bc5-e8c32824be01",
   "metadata": {},
   "outputs": [],
   "source": [
    "diab_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b970fe-7319-4735-8545-500ebe76df14",
   "metadata": {},
   "source": [
    "For simplicity, we will select only the \"Glucose\", \"BloodPressure\", \"Insulin\", and \"BMI\" as feature variables, and Outcome as response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15aa14f-be70-4130-8ac2-84dc993b179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = diab_data[[\"Glucose\", \"BloodPressure\", \"Insulin\", \"BMI\"]]\n",
    "y = diab_data[\"Outcome\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc304c84-5aa4-46dd-a5b3-884649b759ae",
   "metadata": {},
   "source": [
    "We will also do a train-test split:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce7778b-0c65-41d2-9475-668f6896246a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=8532)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe138c13-bbc1-4a9c-8598-ee83bfa798f1",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "\n",
    "We need to load the decision tree classifier from Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c96ce9-7e52-4d1c-8f94-9ad01cc5beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace1cb73-3f82-4130-9659-03b5c03a08df",
   "metadata": {},
   "source": [
    "The procedure of training a Decision in Scikit-learn is similar to the procedures for training other models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb22b69-5ac5-4cdf-a093-f158452ee54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5eba82f-6ce3-4b73-9754-2e00e9459b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = dtree.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c4f28a-1a7c-497e-909e-b3cdd4915bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = dtree.predict(X_train)\n",
    "y_pred_test = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cfe138-b9c3-44e5-b22f-004ee51329c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_train, y_pred_train)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d16bdcd-a6e7-4141-b869-3bb4d383d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dc2428-1cd0-4312-807a-7fa00f109362",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564b025-c2de-4449-9901-7242bf2b27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909237d9-5293-409b-8c33-816c3951149a",
   "metadata": {},
   "source": [
    "There is no doubt that the decision tree clearly overfitted!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70992a5c-1fc2-409e-9de0-e400a51ba605",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb71bd9a-0fc3-43bd-b580-5c7f64e57b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fe2850-2a45-4ad8-8b46-5eb7b19c0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6aae09-d28a-4c28-bdca-592b438a03bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353080c-82bb-44e8-9187-f20124d6edb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f87eadf-dcc2-4571-8492-06d15d7fcbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e89e6-4c95-42fb-a7d3-7a31192c9c3a",
   "metadata": {},
   "source": [
    "### Visualizing a decision tree\n",
    "\n",
    "We can also visualize our decision tree here in the jupyter notebook. Note that decisions trees can become quite big and complex, but it can still be valuable to try to visualize them. To do this, the modules `graphviz` and `pydotplus` needs to be installed into your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c240eff-bdce-4169-8b3d-fcd7090180a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "#from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bbcba8-35f2-4321-a4fa-06035b0e2fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"Glucose\", \"BloodPressure\", \"Insulin\", \"BMI\"]\n",
    "\n",
    "export_graphviz(dtree,\n",
    "                out_file='DT_diabetes.dot',  \n",
    "                filled=True,\n",
    "                rounded=True,\n",
    "                special_characters=True,\n",
    "                feature_names = feature_cols,\n",
    "                class_names=['0','1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfa25d5-02d1-45d7-a1bf-e58e5527996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! dot -Tpng DT_diabetes.dot -o DT_diabetes.png\n",
    "\n",
    "from IPython import display\n",
    "display.Image(\"DT_diabetes.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4507573-b985-41d4-bf37-917bc8ad2c90",
   "metadata": {},
   "source": [
    "### Hyper-parameter tuning\n",
    "\n",
    "As we saw the decision tree completely overfitted to the training data! Luckily, there are several hyper-parameters we can tune to make it overfit less. Two of them are:\n",
    "\n",
    "* **criterion**: Measure of the quality of a split of a column. Mainly *gini* and *entropy*\n",
    "* **max_depth**: The maximum depth of the tree\n",
    "\n",
    "We can use cross-validation with different values for `criterion` and `max_depth` to determine the potential optimal tune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e86a8bc-d34a-4202-bed3-d55d9d56fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterions = [\"gini\", \"entropy\"]\n",
    "maxDepth = range(1, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b447a-33ae-4c9a-8da9-9de1db1b0e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "CVlist = []\n",
    "for c in criterions:\n",
    "    for md in maxDepth:\n",
    "        # Model intializing\n",
    "        dtree = DecisionTreeClassifier(criterion=c, max_depth=md)\n",
    "        # Cross-validation\n",
    "        scores = cross_val_score(dtree, X_train, y_train, cv = 10)\n",
    "        CVlist.append({\"Criteria\": c, \"Max depth\": md, \"CV accuracy\": scores.mean()})   \n",
    "CV_df = pd.DataFrame(CVlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236698a8-3c73-40d6-85e1-880102d0aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3ddf7-7faf-48cb-85a8-9128c4cd446d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=CV_df, x=\"Max depth\", y=\"CV accuracy\", hue=\"Criteria\", style=\"Criteria\", markers=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a55e24-98b0-4af9-99b5-d17cd53e876e",
   "metadata": {},
   "source": [
    "It looks like the best decision tree is obtained by setting max depth to 2 and the spliting criteria to \"entropy\", so let us try to do that and evaluate the model on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ac931e-8cbc-4649-bb42-f22cf28cbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = DecisionTreeClassifier(criterion=\"entropy\", max_depth=2)\n",
    "dtree = dtree.fit(X_train, y_train)\n",
    "y_pred_train = dtree.predict(X_train)\n",
    "y_pred_test = dtree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb51a5d-9ccb-4b79-aee1-786f5781cbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d3fd4b-263b-4147-bc19-7db39174bd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfa9ee-f802-4062-b840-db2d31040a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a98014-351c-4efc-a55d-16471bd59063",
   "metadata": {},
   "source": [
    "There is clearly less overfitting! However, there might be some issue with class balance as only 28 are predicted to have diabetes, while 126 are predicted to not having diabetes. This also indicated by the decent precision, but low recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b21399-6d60-458f-ba8a-1d1b84c5337c",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae83941-e793-467d-9159-baded138633e",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4fccba-093e-470a-9ba6-d8b172fe3d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c763aa43-75be-418b-a410-466749d6e0d7",
   "metadata": {},
   "source": [
    "Let us visualize this tree as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc2563-4825-47b5-be74-59d3b0029aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [\"Glucose\", \"BloodPressure\", \"Insulin\", \"BMI\"]\n",
    "\n",
    "export_graphviz(dtree,\n",
    "                out_file='DT_diabetes_tuned.dot',  \n",
    "                filled=True,\n",
    "                rounded=True,\n",
    "                special_characters=True,\n",
    "                feature_names = feature_cols,\n",
    "                class_names=['0','1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aacfd2f-5c26-4556-9cf6-23ba23826c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "! dot -Tpng DT_diabetes_tuned.dot -o DT_diabetes_tuned.png\n",
    "\n",
    "from IPython import display\n",
    "display.Image(\"DT_diabetes_tuned.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce17b60-9532-4eee-a3a2-7332444f4097",
   "metadata": {},
   "source": [
    "Now, it is not a perfect tree, but certainly easier to grasp, due to its smaller size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3a125b-e2b2-4a5a-90d8-1a08a936dfaf",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "\n",
    "We will now try a Random Forest model. to do this we first have to import the right class from Scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06a0d79-5dd8-49ea-98e3-440da705dbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f33e7a-4f72-40db-acfe-727df0ed0d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier() \n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_train = rf_model.predict(X_train)\n",
    "y_pred_test = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c33b830-a148-4ea3-b631-a6f66a9a02ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f233f4-84fe-475f-8b75-bdfecba24802",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09eb79d4-13c6-4012-a57f-71c4b16e6619",
   "metadata": {},
   "source": [
    "It looks like we clearly overfitted again. However, usually Random Forest models are not that prone to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8e9d6-3135-4b8d-98e6-2aa514cec9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_train, y_pred_train)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4a6f8f-a8ca-4d0a-a63d-6dca50d71cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d443b6-77f8-4848-aada-6f8810360e6f",
   "metadata": {},
   "source": [
    "It looks like we have to do some hyper-parameter tuning again. For Random forest there are many hyper-parameters we can tune, but we will only look at two, the number of trees (called `n_estimators` in `RandomForestClassifier`) and the max depth (called `max_depth` in `RandomForestClassifier`). (Other important hyper-parameters one could tune is the minimum number of samples required to split a note and the number of features considered at each split.) We will do this using cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1302a28-bb97-48bf-834c-4ba693ce4e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrees = [200, 300, 400, 600, 800]\n",
    "maxDepth = [1, 2, 3, 4, 6, 8, 10, 12]\n",
    "\n",
    "CVlist = []\n",
    "for nt in nTrees:\n",
    "    print(\"Trying \", nt, \" number of trees\")\n",
    "    for md in maxDepth:\n",
    "        print(\"Trying a max depth of \", md)\n",
    "        # Model intializing\n",
    "        rf_model = RandomForestClassifier(n_estimators=nt, max_depth=md)\n",
    "        # Cross-validation\n",
    "        scores = cross_val_score(rf_model, X_train, y_train, cv = 10)\n",
    "        CVlist.append({\"Number of trees\": nt, \"Max depth\": md, \"CV accuracy\": scores.mean()})   \n",
    "CV_df = pd.DataFrame(CVlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6573052-a1ba-4896-b8f3-90020209898a",
   "metadata": {},
   "source": [
    "Note that this took a long time! We had 5 different number of trees and 8 different number of depth, which means we have to search through 40 combinations of these two hyper-parameters, and for each of these combinations, we trained 10 models (due to 10-fold cross-validation). Thus, in total we just trained 400 models!!! In the class on \"Improving and selecting machine learning models\", we will talk more about alternatives to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbed0e57-4589-4069-bb4a-838c68698ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(data=CV_df, x=\"Max depth\", y=\"CV accuracy\", hue=\"Number of trees\", style=\"Number of trees\", markers=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c2d3b2-95a6-4649-a9fe-a6b2d4759aa0",
   "metadata": {},
   "source": [
    "It looks like good option for `n_estimators` and `max_depth` is 800 and 4. Thus, we train a new model on all the training data with these hyper-parameter values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8e1169-4d52-4a93-af2b-365c9a06a151",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_final = RandomForestClassifier(n_estimators = 800, max_depth = 4) \n",
    "rf_model_final.fit(X_train, y_train)\n",
    "y_pred_train = rf_model_final.predict(X_train)\n",
    "y_pred_test = rf_model_final.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d5a6e1-6809-4aa5-88cf-d5c879fec577",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = accuracy_score(y_train, y_pred_train)\n",
    "accuracy_test = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Train accuracy:\", accuracy_train)\n",
    "print(\"Test accuracy:\", accuracy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6e78b0-97ba-49b3-b8a6-7f01f5614ead",
   "metadata": {},
   "source": [
    "We see that the model no longer overfit as much, but it has not improved the test accuracy that much. Let us look at the confusion matrix on the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4291bcf7-17a1-493f-bfed-806a94567617",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a984b6c6-9036-419d-8f48-fda935453e3e",
   "metadata": {},
   "source": [
    "Here we see that we manage to move two false positives into true negatives, but that is all that has changed. Let us also calculate the other evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4232a8-50ed-44d5-a0f4-682d00c42c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred_test)\n",
    "recall = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc6775-3309-463c-868c-8fcb7a6cfbe4",
   "metadata": {},
   "source": [
    "Let us finally look at the variable importance to tell us which feature variables are most important in the model's predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bab39f-63af-4380-b3fc-574e5046e5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the feature importances\n",
    "feature_importances = pd.Series(rf_model_final.feature_importances_, index=X_train.columns).sort_values(ascending=True)\n",
    "\n",
    "# Plotting the feature impartances\n",
    "feature_importances.plot.barh()\n",
    "plt.title(\"Variable importance for the final Random Forest classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57897122-524d-4d78-8d97-3c29edbadc9f",
   "metadata": {},
   "source": [
    "We see that the Glucose level is clearly the most important feature followed by BMI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a062739a-8dd4-4e38-b6da-01eaec921c5a",
   "metadata": {},
   "source": [
    "### AdaBoost \n",
    "\n",
    "AdaBoost is an example of a powerful boosting algorithm. To use it we need to import the `AdaBoostClassifier` from `sklearn.ensemble`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47ca825-86dd-44c7-9d83-f913c349d2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee67b446-b797-493d-9cf3-e9e72fcdc5d7",
   "metadata": {},
   "source": [
    "We can now train an AdaBoost model the way we trained the random forest model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454fb33-3ffd-453e-b16d-499f31922101",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_model = AdaBoostClassifier() \n",
    "ab_model.fit(X_train, y_train)\n",
    "y_pred_train = ab_model.predict(X_train)\n",
    "y_pred_test = ab_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6f7946-1be2-4efd-b4f2-c27778ea955c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38f9ce-7a29-4abb-8756-69d6b90bb62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fab921-bba3-4f6d-b71d-864b47f5b5c7",
   "metadata": {},
   "source": [
    "The AdaBoost model does not overfit that much out of the box! Let us look at the Confusion matrix for the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de060aee-d87d-435e-adcc-55ed832cc401",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a0ecf-3af5-4885-b33b-dd978b031c63",
   "metadata": {},
   "source": [
    "It makes 3 more false positive than our final Random Forest model, but also 4 less false negatives. Let us also calculate the other evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b075822-576e-495c-9b1b-f1e00e095b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred_test)\n",
    "recall = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3155013d-9c10-48e5-998c-8de37ed83e3c",
   "metadata": {},
   "source": [
    "It achieved sligtly lower precision, but substainlly higher recall and thereby also a higher f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b66b4a-bfdb-411a-948c-d0ba3b0bb2aa",
   "metadata": {},
   "source": [
    "Let us look at the variable importance also:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e916dbd-0e11-4143-a173-2c13bf11e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the feature importances\n",
    "feature_importances = pd.Series(ab_model.feature_importances_, index=X_train.columns).sort_values(ascending=True)\n",
    "\n",
    "# Plotting the feature impartances\n",
    "feature_importances.plot.barh()\n",
    "plt.title(\"Variable importance for the AdaBoost classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc64d8b7-b234-4aa2-84e7-43b5326321b8",
   "metadata": {},
   "source": [
    "Here we get almost the same as for the Random Forest except that the Insulin feature seems not to be used at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41170188-6b70-453f-afc1-c9ecf282e26c",
   "metadata": {},
   "source": [
    "If you want, you can try to tune some of AdaBoostøs hyper-parameters on your own. Some important hyper-parameters are:\n",
    "* **n_estimators**: Number of trees that are trained iteratively.\n",
    "* **learning_rate**: It contributes to the weights of the trees. It uses 1 as a default value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646ee0e8-43f0-4ae4-a1b3-9c1fa8867300",
   "metadata": {},
   "source": [
    "### XGBoost\n",
    "\n",
    "XGBoost is another powerful and very popular variant of a boosting model based on decision trees. There seems to be no implementation of it in Scikit-learn, so we will use a separate module (also used by Shabab in the class on Time Series Analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbabc625-bbdc-40ad-8a70-712bc14bf2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ff7f1-8fe9-4037-8ca0-a0d7d56f273f",
   "metadata": {},
   "source": [
    "Even though it is not standard Scikit-learn, we can set-up it up the usual way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208ea71c-6a92-4fb6-a964-bf659fa52c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = xgb.XGBClassifier()\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred_train = xgb_model.predict(X_train)\n",
    "y_pred_test = xgb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50036fd3-96b7-4906-9957-8b638f71ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_train, y_pred_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38daf84a-a987-444b-b9b1-fc6c2ee0d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6045983-22c0-4b13-97a1-195bb4e7772c",
   "metadata": {},
   "source": [
    "It clearly overfitted quite a lot, but that is one of the challenges of XGBoost. We will not do hyper-parameter tuning here, but it would be required to get a good model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff718da-03a3-4938-914b-60db4cbaa4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_train, y_pred_train)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda4d3d-a3ff-4573-9d46-10057c99210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred_test)).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0cab72-cae3-424f-b8d9-395ce51e69c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_test, y_pred_test)\n",
    "recall = recall_score(y_test, y_pred_test)\n",
    "f1 = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"f1:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd1699a-18c9-4bae-adb0-147f8e16caed",
   "metadata": {},
   "source": [
    "Clearly here we got a lower precision on our test data. Let us see the variable importance as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b38ef-3fef-4de7-a419-0e0e5a30afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collecting the feature importances\n",
    "feature_importances = pd.Series(xgb_model.feature_importances_, index=X_train.columns).sort_values(ascending=True)\n",
    "\n",
    "# Plotting the feature impartances\n",
    "feature_importances.plot.barh()\n",
    "plt.title(\"Variable importance for the AdaBoost classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64878d80-e00e-46c6-b8ff-a0b003e8d755",
   "metadata": {},
   "source": [
    "It still put Glucose on top, but it has rearanged to features in the bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba2b52-26bd-4add-8960-6fe2091fe589",
   "metadata": {},
   "source": [
    "### Decision trees and ensemble methods for regression\n",
    "\n",
    "As mentioned several times, Decision trees and ensemble methods can also be used for regression. For instance, the Scikit-learn submodule `sklearn.tree` has a class [DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html). There is also a Random Forest class for regression [RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html), and an AdaBoost for regression [AdaBoostRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efc508f-125d-4872-a729-a116b0e823e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
